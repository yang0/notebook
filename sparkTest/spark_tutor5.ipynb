{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本相似度计算 TF-IDF\n",
    "http://dblab.xmu.edu.cn/blog/1766-2/\n",
    "Spark 2.1.0 入门：特征抽取 — TF-IDF(Python版)\n",
    "\n",
    "\n",
    "tf-idf\n",
    "https://baike.baidu.com/link?url=vSHemecmjETsta8JO6IPSN_mDMQdV4d0mIB7KCLrB-XWEq28oggFiBMv0gJQn93jiXMVOC9NAFgtdJ56O5el8a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yang0/installer/spark-2.4.3-bin-hadoop2.7\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.3\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Jul 11 2019 14:32:02)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "spark_home = os.environ.get('SPARK_HOME',None)\n",
    "print(spark_home)\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.10.4-src.zip'))\n",
    "\n",
    "exec(open(os.path.join(spark_home,'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer_e44ba1b14916\n",
      "[Row(label=0, sentence='I heard about Spark and I love Spark', words=['i', 'heard', 'about', 'spark', 'and', 'i', 'love', 'spark']), Row(label=0, sentence='I wish Java could use case classes', words=['i', 'wish', 'java', 'could', 'use', 'case', 'classes']), Row(label=1, sentence='Logistic regression models are neat', words=['logistic', 'regression', 'models', 'are', 'neat'])]\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0|(20,[0,5,9,13,17]...|\n",
      "|    0|(20,[2,7,9,13,15]...|\n",
      "|    1|(20,[4,6,13,15,18...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF,IDF,Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([(0, \"I heard about Spark and I love Spark\"),(0, \"I wish Java could use case classes\"),(1, \"Logistic regression models are neat\")]).toDF(\"label\", \"sentence\")\n",
    "\n",
    "#用tokenizer对句子进行分词\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "print(tokenizer)\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "print(wordsData.collect())\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData) #分词序列被变换成一个稀疏特征向量\n",
    "\n",
    "# 使用IDF来对单纯的词频特征向量进行修正，使其更能体现不同词汇对文本的区别能力，\n",
    "# IDF是一个Estimator，调用fit()方法并将词频向量传入，即产生一个IDFModel\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "rescaledData.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "官方文档\n",
    "https://spark.apache.org/docs/latest/ml-features.html#tf-idf\n",
    "\n",
    "Pyspark Word2Vec + jieba 训练词向量流程\n",
    "https://zhuanlan.zhihu.com/p/60532089\n",
    "\n",
    "Spark MLlib实现的中文文本分类–Naive Bayes\n",
    "http://lxw1234.com/archives/2016/01/605.htm\n",
    "\n",
    "于Spark的TF-IDF算法的中文文本相似度实现\n",
    "https://blog.csdn.net/cap3396g/article/details/79256625\n",
    "\n",
    "使用spark计算文档相似度\n",
    "https://blog.csdn.net/weixin_34413103/article/details/92699339"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**中文分词**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['这是', '医保', '板蓝根']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "DICT_LIST = [\"./jieba_dict/medicine.dict\", \"./jieba_dict/user.dict\"]\n",
    "STOP_WORDS = \"./jieba_dict/stopwords.txt\"\n",
    "\n",
    "stopwordset = set()\n",
    "\n",
    "# 加载用户字典\n",
    "def loadDict():\n",
    "    loadStopWords()\n",
    "    loadUserDicts()\n",
    "\n",
    "\n",
    "def loadUserDicts():\n",
    "    for dict in DICT_LIST:\n",
    "        jieba.load_userdict(dict)\n",
    "\n",
    "def loadUserDict():\n",
    "    jieba.load_userdict(USER_DICT)\n",
    "\n",
    "#定义停词\n",
    "def loadStopWords():\n",
    "    global stopwordset\n",
    "    with open(STOP_WORDS, 'r', encoding='utf-8') as sw:\n",
    "        stopwordset = sw.readlines()\n",
    "        stopwordset = [x.strip() for x in stopwordset]\n",
    "\n",
    "def cutWords(s, cutAll=False):\n",
    "    loadDict()\n",
    "    words = jieba.cut(s, cut_all=cutAll)\n",
    "    words = [w for w in words if w not in stopwordset]\n",
    "    return words\n",
    "#     return ' '.join(words)\n",
    "\n",
    "print(cutWords(\"你好，这是医保板蓝根\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- file: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "# rdd方式的读取\n",
    "# 第一次toDF要先推导字段类型，第二次再指定字段名称\n",
    "cnData = sc.wholeTextFiles(\"./cn_data/\").toDF().toDF(\"file\",\"text\")\n",
    "cnData.printSchema()\n",
    "\n",
    "cnData = cnData.withColumn('words',cutWords_udf(cnData['text']))\n",
    "cnData.first()\n",
    "\n",
    "#计算词频\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "featurizedData = hashingTF.transform(cnData) #分词序列被变换成一个稀疏特征向量\n",
    "\n",
    "#计算TF-IDF值\n",
    "# 如果有一个词语只在a文档出现，在其他文档出现的次数少，那么该词的idf值就高，适合分类\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "rescaledData.select(\"file\", \"features\").count()\n",
    "\n",
    "rows = rescaledData.select(\"file\", \"features\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:/mnt/e/testProject/sparkTest/cn_data/1.txt\n",
      "file:/mnt/e/testProject/sparkTest/cn_data/2.txt\n",
      "file:/mnt/e/testProject/sparkTest/cn_data/3.txt\n",
      "file:/mnt/e/testProject/sparkTest/cn_data/4.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for row in rows:\n",
    "    print(row.file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3691.1335557440075\n",
      "1181.8652551579291\n",
      "1530.2595263280284\n",
      "2793.8133277847924\n"
     ]
    }
   ],
   "source": [
    "print(rows[0].features.squared_distance(rows[1].features))\n",
    "print(rows[0].features.squared_distance(rows[2].features))\n",
    "print(rows[0].features.squared_distance(rows[3].features))\n",
    "print(rows[1].features.squared_distance(rows[2].features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.980447656477595\n",
      "0.9909107174983364\n",
      "0.9895697103885169\n",
      "0.9819028177096234\n"
     ]
    }
   ],
   "source": [
    "# 余弦相似度\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "print(cosine(rows[0].features.toArray(), rows[1].features.toArray()))\n",
    "print(cosine(rows[0].features.toArray(), rows[2].features.toArray()))\n",
    "print(cosine(rows[0].features.toArray(), rows[3].features.toArray()))\n",
    "print(cosine(rows[1].features.toArray(), rows[3].features.toArray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark协同推荐，2.4.3版本\n",
    "https://spark.apache.org/docs/latest/ml-collaborative-filtering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yang0/installer/spark-2.4.3-bin-hadoop2.7\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.3\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Jul 11 2019 14:32:02)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "spark_home = os.environ.get('SPARK_HOME',None)\n",
    "print(spark_home)\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.10.4-src.zip'))\n",
    "\n",
    "exec(open(os.path.join(spark_home,'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.4927293118707536\n",
      "[Row(movieId=65, recommendations=[Row(userId=23, rating=4.835751533508301), Row(userId=12, rating=4.284965515136719), Row(userId=11, rating=3.461029052734375), Row(userId=9, rating=3.2582976818084717), Row(userId=17, rating=2.8085155487060547), Row(userId=29, rating=2.0837039947509766), Row(userId=15, rating=2.053344249725342), Row(userId=26, rating=1.97727370262146), Row(userId=5, rating=1.9405314922332764), Row(userId=3, rating=1.9380824565887451)]), Row(movieId=26, recommendations=[Row(userId=16, rating=3.8295087814331055), Row(userId=10, rating=3.2082841396331787), Row(userId=4, rating=3.1849899291992188), Row(userId=0, rating=2.9807891845703125), Row(userId=15, rating=2.7445921897888184), Row(userId=25, rating=2.538245439529419), Row(userId=11, rating=2.2013213634490967), Row(userId=26, rating=1.8865208625793457), Row(userId=27, rating=1.4297059774398804), Row(userId=29, rating=1.4207522869110107)]), Row(movieId=29, recommendations=[Row(userId=7, rating=5.200272560119629), Row(userId=21, rating=4.784268856048584), Row(userId=26, rating=4.470052242279053), Row(userId=14, rating=4.33486270904541), Row(userId=8, rating=4.105813026428223), Row(userId=4, rating=4.044369697570801), Row(userId=16, rating=4.026528835296631), Row(userId=2, rating=3.9125735759735107), Row(userId=10, rating=3.499850034713745), Row(userId=22, rating=3.172478199005127)])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "\n",
    "lines = spark.read.text(\"./als_data/sample_movielens_ratings.txt\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\"::\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=int(p[3])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "# 训练\n",
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "\n",
    "# Generate top 10 movie recommendations for each user\n",
    "# 为所有用户推荐电影\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "\n",
    "# Generate top 10 user recommendations for each movie\n",
    "# 为所有电影推荐用户\n",
    "movieRecs = model.recommendForAllItems(10)\n",
    "\n",
    "# Generate top 10 movie recommendations for a specified set of users\n",
    "# 三个用户，每个用户十部电影\n",
    "users = ratings.select(als.getUserCol()).distinct().limit(3)\n",
    "userSubsetRecs = model.recommendForUserSubset(users, 10)\n",
    "\n",
    "# Generate top 10 user recommendations for a specified set of movies\n",
    "# 三部电影，每部前十推荐用户\n",
    "movies = ratings.select(als.getItemCol()).distinct().limit(3)\n",
    "movieSubSetRecs = model.recommendForItemSubset(movies, 10)\n",
    "print(movieSubSetRecs.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推荐算法——潜在因子(Latent Factor)算法\n",
    "https://www.jianshu.com/p/f10d15ac6a80\n",
    "\n",
    "推荐引擎里面建立标签系统是关键。标签需要有权重，权重最好能变化。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
